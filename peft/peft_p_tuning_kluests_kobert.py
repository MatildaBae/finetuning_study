# -*- coding: utf-8 -*-
"""PEFT_P-Tuning_KlueSts_Kobert.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nggYADJESm0djwu2n5SwIwHPvE84c9VX

# P-Tuning semantic similarity 예제 - MRPC 데이터셋
## 작성자 : AISchool ([http://aischool.ai/](http://aischool.ai/%ec%98%a8%eb%9d%bc%ec%9d%b8-%ea%b0%95%ec%9d%98-%ec%b9%b4%ed%85%8c%ea%b3%a0%eb%a6%ac/) )
## Reference : https://huggingface.co/docs/peft/task_guides/ptuning-seq-classification

# 필요한 라이브러리 설치
"""

!pip install -q peft transformers datasets evaluate

"""# 설정값 지정"""

from transformers import (
    AutoModelForSequenceClassification,
    AutoTokenizer,
    DataCollatorWithPadding,
    TrainingArguments,
    Trainer,
)
from peft import (
    get_peft_config,
    get_peft_model,
    get_peft_model_state_dict,
    set_peft_model_state_dict,
    PeftType,
    PromptEncoderConfig,
)
from datasets import load_dataset
import evaluate
import torch

model_name_or_path = "roberta-large"
task = "mrpc"
num_epochs = 5
lr = 1e-3
batch_size = 32

!pip install peft==0.11.0

"""# MRPC 데이터셋 불러오기
## (GLUE 벤치마크 데이터셋의 부분집합 데이터셋으로 **두개의 문장이 의미론적으로 동일한지 다른지를 측정**하는 데이터셋입니다.)
"""

dataset = load_dataset("glue", task)
dataset["train"][0]

"""# F1 scroe 측정을 위한 metric을 정의합니다."""

metric = evaluate.load("glue", task)

import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=labels)

"""# Tokenizer 불러오기 & 데이터셋 전처리"""

if any(k in model_name_or_path for k in ("gpt", "opt", "bloom")):
    padding_side = "left"
else:
    padding_side = "right"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)
if getattr(tokenizer, "pad_token_id") is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id


def tokenize_function(examples):
    # max_length=None => use the model max length (it's actually the default)
    outputs = tokenizer(examples["sentence1"], examples["sentence2"], truncation=True, max_length=None)
    return outputs

tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["idx", "sentence1", "sentence2"],
)

tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

tokenized_datasets

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")

"""# PEFT 모델 설정"""

peft_config = PromptEncoderConfig(task_type="SEQ_CLS", num_virtual_tokens=20, encoder_hidden_size=128)

model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)

"""# P-Tuning 기법으로 인해 전체 모델의 0.67%의 파라미터만 Fine-Tuning에 사용"""

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

"""# Training 시작"""

training_args = TrainingArguments(
    output_dir="roberta-large-peft-p-tuning",
    learning_rate=lr,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=num_epochs,
    weight_decay=0.01,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

trainer.train()

"""# 학습이 끝난 모델 불러오기"""

import torch
from peft import PeftModel, PeftConfig
from transformers import AutoModelForCausalLM, AutoTokenizer

peft_model_id = "./roberta-large-peft-p-tuning/checkpoint-575"
config = PeftConfig.from_pretrained(peft_model_id)
inference_model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
model = PeftModel.from_pretrained(inference_model, peft_model_id)

"""# 학습이 끝난 모델을 sample text에 대한 Inference"""

classes = ["not equivalent", "equivalent"]

sentence1 = "Coast redwood trees are the tallest trees on the planet and can grow over 300 feet tall."
sentence2 = "The coast redwood trees, which can attain a height of over 300 feet, are the tallest trees on earth."

inputs = tokenizer(sentence1, sentence2, truncation=True, padding="longest", return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs).logits
    print(outputs)

paraphrased_text = torch.softmax(outputs, dim=1).tolist()[0]
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrased_text[i] * 100))}%")

"""# Klue Sts Kobert Ptuning"""

model_name_or_path = "monologg/kobert"
task = "sts"
num_epochs = 1
lr = 1e-3
batch_size = 32

dataset = load_dataset("klue", task)
dataset["train"][0]

metric = evaluate.load("glue", "stsb")

import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.squeeze(predictions)
    return metric.compute(predictions=predictions, references=labels)

if any(k in model_name_or_path for k in ("gpt", "opt", "bloom")):
    padding_side = "left"
else:
    padding_side = "right"

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, padding_side=padding_side)
if getattr(tokenizer, "pad_token_id") is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id

def tokenize_function(examples):
    outputs = tokenizer(
        examples["sentence1"],
        examples["sentence2"],
        truncation=True,
        padding='max_length',
        max_length=128)
    # Convert labels to the binary-label integer value
    outputs["labels"] = [label['binary-label'] for label in examples['labels']]
    return outputs

tokenized_datasets = dataset.map(
    tokenize_function,
    batched=True,
    remove_columns=["guid", "source", "sentence1", "sentence2"],
)

tokenized_datasets['train'][0]

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding="longest")

peft_config = PromptEncoderConfig(task_type="SEQ_CLS", num_virtual_tokens=20, encoder_hidden_size=128)

model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path, return_dict=True)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# Save strategy를 "no"로 설정해 저장하는 과정을 피함
training_args = TrainingArguments(
    output_dir="./results",
    do_train=True,
    do_eval=True,
    save_strategy="no",  # Checkpoint 저장을 방지
    num_train_epochs=5,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    logging_dir="./logs",
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    tokenizer=tokenizer,  # 이 부분을 포함해도 저장 과정에서 무시될 수 있게 설정됨
    compute_metrics=compute_metrics,
)

trainer.train()

classes = ["not equivalent", "equivalent"]

sentence1 = "안녕 좋은 하루! 사랑해"
sentence2 = "응응 너도 좋은 하루 나도 사랑해"

inputs = tokenizer(sentence1, sentence2, truncation=True, padding="longest", return_tensors="pt")

import torch

# 모델을 GPU로 이동
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# 입력 데이터 생성
inputs = tokenizer(sentence1, sentence2, truncation=True, padding="longest", return_tensors="pt")

# 입력 텐서를 모델이 있는 장치로 이동
inputs = {key: value.to(device) for key, value in inputs.items()}

# 예측
with torch.no_grad():
    outputs = model(**inputs).logits
    print(outputs)

# 소프트맥스 계산 후 출력
paraphrased_text = torch.softmax(outputs, dim=1).tolist()[0]
for i in range(len(classes)):
    print(f"{classes[i]}: {int(round(paraphrased_text[i] * 100))}%")

import os
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# 모델 저장 디렉토리 지정
output_dir = "sts_final_model"

# 디렉토리가 없으면 생성
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

# 모델과 토크나이저 저장
tokenizer.save_vocabulary(output_dir)
model.save_pretrained(output_dir)

print(f"Model and tokenizer saved to {output_dir}")

import os
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# 모델 저장 디렉토리 지정
output_dir = "sts_final_model"

# 나중에 모델과 토크나이저를 불러오는 코드
loaded_tokenizer = AutoTokenizer.from_pretrained(output_dir)
loaded_model = AutoModelForSequenceClassification.from_pretrained(output_dir)

print("Model and tokenizer loaded successfully")