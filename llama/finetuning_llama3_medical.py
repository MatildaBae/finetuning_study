# -*- coding: utf-8 -*-
"""FineTuning_Llama3_Medical.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ozfxlefX2Dg1_3mGenk0Z2hdSO4XXKr_
"""

from google.colab import drive
drive.mount('/content/drive')

"""## 데이터 정제"""

import os
import json
from tqdm import tqdm

# JSON 파일을 읽고 QnA 데이터 추출하는 함수
def extract_qna_from_json(file_path, is_question_file):
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if is_question_file:
        # 질문이 있는 경우, 'question' 키에서 질문을 추출
        return data.get("question", "")
    else:
        # 답변이 있는 경우, 'answer' 내의 intro, body, conclusion 결합
        answer_intro = data.get("answer", {}).get("intro", "")
        answer_body = data.get("answer", {}).get("body", "")
        answer_conclusion = data.get("answer", {}).get("conclusion", "")
        return answer_intro + " " + answer_body + " " + answer_conclusion

# 주어진 폴더에서 모든 JSON 파일을 재귀적으로 탐색하는 함수
def collect_qna_data(question_dir, answer_dir, output_file):
    # 주어진 output_file에 이어서 데이터를 쓸 수 있도록 'a' 모드로 연다.
    with open(output_file, "a", encoding="utf-8") as f:
        # 의도별로 탐색
        for category in tqdm(os.listdir(question_dir), desc="Processing Categories", unit="category"):
            category_path = os.path.join(question_dir, category)
            if os.path.isdir(category_path):
                # 의도별 질문 파일 리스트
                question_files = []
                for root, dirs, files in os.walk(category_path):
                    for file in files:
                        if file.endswith(".json"):
                            question_files.append(os.path.join(root, file))

                # 같은 의도의 답변 폴더 탐색
                answer_category_path = os.path.join(answer_dir, category)
                if os.path.isdir(answer_category_path):
                    answer_files = []
                    for root, dirs, files in os.walk(answer_category_path):
                        for file in files:
                            if file.endswith(".json"):
                                answer_files.append(os.path.join(root, file))

                    # tqdm을 사용하여 파일 개수로 진행 표시
                    total_files = min(len(question_files), len(answer_files))
                    for i in tqdm(range(total_files), desc=f"Processing QnA in {category}", unit="pair"):
                        question = extract_qna_from_json(question_files[i], is_question_file=True)
                        answer = extract_qna_from_json(answer_files[i], is_question_file=False)

                        # QnA 데이터 형식으로 작성
                        qa_pair = f"Q: {question}\nA: {answer}\n\n"

                        # 각 QnA를 파일에 바로 기록
                        f.write(qa_pair)

    print("학습 데이터셋이 llama3_training_data.txt로 저장되었습니다.")

# 질문 폴더와 답변 폴더 경로 설정
question_folder = "data/1.질문"  # 질문 폴더 경로
answer_folder = "data/2.답변"  # 답변 폴더 경로
output_file = "llama3_training_data.txt"  # 출력 파일 경로

# QnA 데이터 수집
collect_qna_data(question_folder, answer_folder, output_file)

import os
import json
from tqdm import tqdm

# JSON 파일을 읽고 QnA 데이터 추출하는 함수
def extract_qna_from_json(file_path, is_question_file):
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    if is_question_file:
        # 질문이 있는 경우, 'question' 키에서 질문을 추출
        return data.get("question", "")
    else:
        # 답변이 있는 경우, 'answer' 내의 intro, body, conclusion 결합
        answer_body = data.get("answer", {}).get("body", "")
        return answer_body

# 주어진 폴더에서 모든 JSON 파일을 재귀적으로 탐색하는 함수
def collect_qna_data(question_dir, answer_dir, output_file):
    # 주어진 output_file에 이어서 데이터를 쓸 수 있도록 'a' 모드로 연다.
    with open(output_file, "a", encoding="utf-8") as f:
        # 의도별로 탐색
        for category in tqdm(os.listdir(question_dir), desc="Processing Categories", unit="category"):
            category_path = os.path.join(question_dir, category)
            if os.path.isdir(category_path):
                # 의도별 질문 파일 리스트
                question_files = []
                for root, dirs, files in os.walk(category_path):
                    for file in files:
                        if file.endswith(".json"):
                            question_files.append(os.path.join(root, file))

                # 같은 의도의 답변 폴더 탐색
                answer_category_path = os.path.join(answer_dir, category)
                if os.path.isdir(answer_category_path):
                    answer_files = []
                    for root, dirs, files in os.walk(answer_category_path):
                        for file in files:
                            if file.endswith(".json"):
                                answer_files.append(os.path.join(root, file))

                    # tqdm을 사용하여 파일 개수로 진행 표시
                    total_files = min(len(question_files), len(answer_files))
                    for i in tqdm(range(total_files), desc=f"Processing QnA in {category}", unit="pair"):
                        question = extract_qna_from_json(question_files[i], is_question_file=True)
                        answer = extract_qna_from_json(answer_files[i], is_question_file=False)

                        # QnA 데이터 형식으로 작성
                        qa_pair = f"Q: {question}\nA: {answer}\n\n"

                        # 각 QnA를 파일에 바로 기록
                        f.write(qa_pair)

    print("학습 데이터셋이 llama3_training_data.txt로 저장되었습니다.")

# 질문 폴더와 답변 폴더 경로 설정
question_folder = "/Users/jiwonbae/Downloads/Training/02.라벨링데이터/1.질문"  # 질문 폴더 경로
answer_folder = "/Users/jiwonbae/Downloads/Training/02.라벨링데이터/2.답변"  # 답변 폴더 경로
output_file = "llama3_training_data.txt"  # 출력 파일 경로

# QnA 데이터 수집
collect_qna_data(question_folder, answer_folder, output_file)

import json

# txt 파일을 읽어 QnA 데이터를 JSON 형식으로 변환
def convert_txt_to_json(input_txt_file, output_json_file):
    # txt 파일 읽기
    with open(input_txt_file, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # QnA 데이터를 JSON 형식으로 변환
    qna_data = []
    question = None
    answer = None

    for line in lines:
        line = line.strip()
        if line.startswith("Q:"):
            if question and answer:
                # 이전 질문과 답변을 리스트에 추가
                qna_data.append({"question": question, "answer": answer})
            # 새로운 질문을 시작
            question = line[2:].strip()
        elif line.startswith("A:"):
            answer = line[2:].strip()

    # 마지막에 남아 있는 질문과 답변을 추가
    if question and answer:
        qna_data.append({"question": question, "answer": answer})

    # JSON 파일로 저장
    with open(output_json_file, 'w', encoding='utf-8') as f:
        for item in qna_data:
            f.write(json.dumps(item, ensure_ascii=False) + "\n")

    print(f"파일이 {output_json_file}로 저장되었습니다.")

# 입력과 출력 파일 경로 설정
# input_txt_file = 'llama3_training_data_small.txt'  # 기존 txt 파일 경로
# output_json_file = 'llama3_training_data_small.json'  # 변환할 json 파일 경로


# 변환 함수 실행
convert_txt_to_json('llama3_training_data_small.txt', 'llama3_training_data_small.json')
convert_txt_to_json('llama3_training_data.txt', 'llama3_training_data.json')

import json

# 기존 llama3_training_data.json 파일을 읽어오기
def load_llama3_data(input_file):
    with open(input_file, 'r', encoding='utf-8') as f:
        data = f.readlines()
    return data

# 데이터를 Llama3 학습에 적합한 형식으로 변환
def convert_to_finetuning_format(data, num_items=20):
    final_prompt_list = []
    for idx, line in enumerate(data):
        if idx >= num_items:  # 빠른 학습을 위해 20개만 사용
            break
        # 각 항목은 이미 QnA 형식이므로, 질문과 답변을 추출
        entry = json.loads(line)
        question = entry.get('question', '')
        answer = entry.get('answer', '')

        # Llama3에 적합한 프롬프트 형식으로 변환
        prompt = f"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: {question} ### Response: {answer}"
        prompt_dict = {'text': prompt}
        final_prompt_list.append(prompt_dict)

    return final_prompt_list

# 저장 함수
def save_to_json(output_file, data):
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("[\n")
        for i, item in enumerate(data):
            json_str = json.dumps(item, ensure_ascii=False)
            f.write(json_str)
            if i < len(data) - 1:  # 마지막 원소가 아니라면 콤마를 추가
                f.write(",\n")
            else:  # 마지막 원소라면 콤마를 추가하지 않음
                f.write("\n")
        f.write("]\n")

# main
input_file = 'training_data.json'  # 기존 QnA 데이터 파일 경로
output_file = 'data/llama3_training_data.json'  # 변환된 학습 데이터 파일 경로

# 1. 기존 파일 로드
data = load_llama3_data(input_file)

# 2. 데이터를 Llama3 학습에 적합한 형식으로 변환
final_prompt_list = convert_to_finetuning_format(data)

# 3. 변환된 데이터를 JSON 파일로 저장
save_to_json(output_file, final_prompt_list)

print(f"파일이 {output_file}로 저장되었습니다.")

# main
input_file = 'training_data_small.json'  # 기존 QnA 데이터 파일 경로
output_file = 'data/llama3_training_data_small.json'  # 변환된 학습 데이터 파일 경로

# 1. 기존 파일 로드
data = load_llama3_data(input_file)

# 2. 데이터를 Llama3 학습에 적합한 형식으로 변환
final_prompt_list = convert_to_finetuning_format(data)

# 3. 변환된 데이터를 JSON 파일로 저장
save_to_json(output_file, final_prompt_list)

print(f"파일이 {output_file}로 저장되었습니다.")

"""## 튜닝"""

!nvidia-smi

!pip install -q autotrain-advanced

!autotrain setup --update-torch

import os
os.environ['HF_TOKEN']=""

"""## Baseline"""

# Data Augmentation 적용 o
!autotrain llm --train \
    --project-name "llama3-medical-finetuning-da-8B" \
    --model "meta-llama/Meta-Llama-3-8B" \
    --data-path "/content/drive/MyDrive/파인튜닝_의료/train_data" \
    --text-column "text" \
    --peft \
    --quantization "int4" \
    --lr 2e-4 \
    --batch-size 8 \
    --epochs 3 \
    --trainer sft \
    --model_max_length 256

import zipfile
import shutil
from google.colab import files

# 압축할 폴더 이름
#folder_name = "llama2-korquad-finetuning"    # Data Augmentation 적용 x
folder_name = "llama3-medical-finetuning-da-8B"  # Data Augmentation 적용 o

# 생성될 ZIP 파일 이름
#zip_file_name = "llama2-korquad-finetuning.zip"  # Data Augmentation 적용 x
zip_file_name = "llama3-medical-finetuning-da.zip" # Data Augmentation 적용 o

# 폴더를 ZIP 파일로 압축
shutil.make_archive(zip_file_name[:-4], 'zip', folder_name)

# ZIP 파일을 로컬로 다운로드
files.download(zip_file_name)

"""## Inference"""

import zipfile

# 압축 해제할 ZIP 파일 이름
#zip_file_name = "llama2-korquad-finetuning.zip"      # Data Augmentation 적용 x
zip_file_name = "llama3-medical-finetuning-da.zip"  # Data Augmentation 적용 o

# 압축을 해제할 대상 폴더. 이 예시에서는 같은 이름의 폴더에 압축을 해제합니다.
extract_folder_name = "./llama3-medical-finetuning-da"

# ZIP 파일 압축 해제
with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall(extract_folder_name)

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, PeftConfig

model_id = "meta-llama/Meta-Llama-3-8B"
peft_model_id = "/content/llama3-medical-finetuning-da-8B"   # Data Augmentation 적용 o

config = PeftConfig.from_pretrained(peft_model_id)

bnb_config = BitsAndBytesConfig(
    load_in_8bit=False,
    load_in_4bit=True,
    llm_int8_threshold=6.0,
    llm_int8_skip_modules=None,
    llm_int8_enable_fp32_cpu_offload=False,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=False,
    bnb_4bit_compute_dtype="float16",
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})
model = PeftModel.from_pretrained(model, peft_model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)

model.eval()

prompt = "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: %s ### Response: "

# do_sample : False -> 항상 가장 높은 확률값을 가진 단어로 예측
def gen(x):
    q = prompt % (x,)
    gened = model.generate(
        **tokenizer(
            q,
            return_tensors='pt',
            return_token_type_ids=False
        ).to('cuda'),
        max_new_tokens=128,
        early_stopping=False,
        do_sample=True,
    )
    return tokenizer.decode(gened[0]).replace(q, "")

"""/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:638: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
  warnings.warn(

-> 이런 오류로, early_stopping=False, do_sample=True
"""

gen("노안을 예방하려면 어떻게 해야해?")

gen("겨울이라 피부가 많이 건조해져. 어떻게 치료해?")

"""이런 식으로 do_sample=True 했더니 연관이 없는 것들이 나옴"""

prompt = "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: %s ### Response: "

# do_sample : False -> 항상 가장 높은 확률값을 가진 단어로 예측
def gen(x):
    q = prompt % (x,)
    gened = model.generate(
        **tokenizer(
            q,
            return_tensors='pt',
            return_token_type_ids=False
        ).to('cuda'),
        max_new_tokens=128,
        early_stopping=True,
        do_sample=False,
    )
    return tokenizer.decode(gened[0]).replace(q, "")

gen("겨울이라 피부가 많이 건조해져. 어떻게 치료해?")

gen("배 아프고 소화가 안돼. 왜 그런 거야?")

gen("소화불량인 거 같아. 왜 그런 거야?")

"""## 에폭 3배 = 150"""

# Data Augmentation 적용 o
!autotrain llm --train \
    --project-name "llama3-medical-finetuning-da-120" \
    --model "meta-llama/Meta-Llama-3-8B" \
    --data-path "/content/drive/MyDrive/파인튜닝_의료/train_data" \
    --text-column "text" \
    --peft \
    --quantization "int4" \
    --lr 2e-4 \
    --batch-size 8 \
    --epochs 150 \
    --trainer sft \
    --model_max_length 256

import zipfile
import shutil
from google.colab import files

# 압축할 폴더 이름
#folder_name = "llama2-korquad-finetuning"    # Data Augmentation 적용 x
folder_name = "llama3-medical-finetuning-da-120"  # Data Augmentation 적용 o

# 생성될 ZIP 파일 이름
#zip_file_name = "llama2-korquad-finetuning.zip"  # Data Augmentation 적용 x
zip_file_name = "llama3-medical-finetuning-da_long.zip" # Data Augmentation 적용 o

# 폴더를 ZIP 파일로 압축
shutil.make_archive(zip_file_name[:-4], 'zip', folder_name)

# ZIP 파일을 로컬로 다운로드
files.download(zip_file_name)

"""## Inference2"""

import zipfile

# 압축 해제할 ZIP 파일 이름
#zip_file_name = "llama2-korquad-finetuning.zip"      # Data Augmentation 적용 x
zip_file_name = "llama3-medical-finetuning-da_long.zip"  # Data Augmentation 적용 o

# 압축을 해제할 대상 폴더. 이 예시에서는 같은 이름의 폴더에 압축을 해제합니다.
extract_folder_name = "./llama3-medical-finetuning-da_long"

# ZIP 파일 압축 해제
with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:
    zip_ref.extractall(extract_folder_name)

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel, PeftConfig

model_id = "meta-llama/Meta-Llama-3-8B"
peft_model_id = "/content/llama3-medical-finetuning-da-120"   # Data Augmentation 적용 o

config = PeftConfig.from_pretrained(peft_model_id)

bnb_config = BitsAndBytesConfig(
    load_in_8bit=False,
    load_in_4bit=True,
    llm_int8_threshold=6.0,
    llm_int8_skip_modules=None,
    llm_int8_enable_fp32_cpu_offload=False,
    llm_int8_has_fp16_weight=False,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=False,
    bnb_4bit_compute_dtype="float16",
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"":0})
model = PeftModel.from_pretrained(model, peft_model_id)
tokenizer = AutoTokenizer.from_pretrained(model_id)

model.eval()

prompt = "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: %s ### Response: "

# do_sample : False -> 항상 가장 높은 확률값을 가진 단어로 예측
def gen(x):
    q = prompt % (x,)
    gened = model.generate(
        **tokenizer(
            q,
            return_tensors='pt',
            return_token_type_ids=False
        ).to('cuda'),
        max_new_tokens=128,
        early_stopping=True,
        do_sample=False,
    )
    return tokenizer.decode(gened[0]).replace(q, "")

gen("겨울이라 피부가 많이 건조해져. 어떻게 치료해?")

gen("소화불량인 거 같아. 왜 그런 거야?")

gen("장티푸스는 어떻게 예방하나요?")

gen("라임병이 무엇인가요?")

"""오버핏?"""

