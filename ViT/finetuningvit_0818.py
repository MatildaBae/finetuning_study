# -*- coding: utf-8 -*-
"""FineTuningViT_0818.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13yjCqwFNzN89bGnMNIF7t0cVhDqAJUGJ
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Baseline 시작 모델"""

from transformers import TrOCRProcessor, VisionEncoderDecoderModel
from torchsummary import summary

# 모델과 프로세서를 불러옵니다.
model = VisionEncoderDecoderModel.from_pretrained("microsoft/trocr-small-handwritten")
processor = TrOCRProcessor.from_pretrained("microsoft/trocr-small-handwritten")

encoder = model.encoder
print(encoder)

"""### Test"""

import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from PIL import Image
import numpy as np



img_path = '/content/drive/MyDrive/ttrain_set/1001LI-A.jpg'
image = Image.open(img_path)
input = processor(images=image, return_tensors="pt").pixel_values
input.shape

output=encoder(input)
# dir(output)

output[0]
#torch.Size([1, 577, 768])

"""### Baseline : 0.6393"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import copy


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def process_image(image):
    processed_image = processor(images=image, return_tensors="pt").pixel_values
    return processed_image.squeeze(0)

size = (384, 384)

train_data_augmentation = transforms.Compose([
    transforms.Lambda(process_image),
    transforms.RandomResizedCrop(size),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(degrees=2),
    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
])

val_data_augmentation = transforms.Compose([
    transforms.Lambda(process_image),
    transforms.CenterCrop(size),
    # transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Rescaling(scale=1.0 / 127.5, offset=-1) 적용  ...?
])

train_dir = "/content/drive/MyDrive/split_dataset/train"
val_dir = "/content/drive/MyDrive/split_dataset/val"
test_dir = "/content/drive/MyDrive/split_dataset/test"

train_dataset = ImageFolder(
    root=train_dir,
    transform=train_data_augmentation
)
val_dataset = ImageFolder(
    root=val_dir,
    transform=val_data_augmentation
)
test_dataset = ImageFolder(
    root=test_dir,
    transform=transforms.Lambda(process_image)
)

train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

import torch.nn.init as init

class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)  # GlobalAveragePooling1D에 해당
        self.dropout1 = nn.Dropout(0.1)
        self.fc1 = nn.Linear(578, 2048)   # 입력 크기 577, 출력 크기 128
        self.relu = nn.ReLU()
        self.dropout2 = nn.Dropout(0.1)
        self.fc2 = nn.Linear(2048, 1024)  # 입력 크기 577, 출력 크기 128
        self.relu = nn.ReLU()
        self.dropout3 = nn.Dropout(0.1)
        self.fc3 = nn.Linear(1024, 2)  # 2 클래스에 대한 출력

        self._initialize_weights()

    def forward(self, x):
        x = self.global_avg_pool(x).squeeze(-1)  # (batch_size, 578)
        x = self.dropout1(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.dropout3(x)
        x = self.fc3(x)
        return x

    def _initialize_weights(self):
        # He initialization for layers followed by ReLU
        for layer in self.modules():
            if isinstance(layer, nn.Linear):
                init.kaiming_normal_(layer.weight, nonlinearity='relu')
                if layer.bias is not None:
                    init.constant_(layer.bias, 0)

class WholeModel(nn.Module):
    def __init__(self, encoder, classifier):
        super(WholeModel, self).__init__()
        self.encoder = encoder
        self.classifier = classifier

    def forward(self, x):
        x = self.encoder(x)
        x = x[0]
        x = self.classifier(x)
        return x

model = WholeModel(encoder, Classifier).to(device)

classifier = Classifier()
model = WholeModel(encoder, classifier).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=0.005)

from torch.optim.lr_scheduler import StepLR

scheduler = StepLR(optimizer, step_size=10, gamma=0.1)

epochs = 10

train_losses = []
val_losses = []

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item() * labels.size(0)
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
    scheduler.step()

    train_loss = running_loss / total
    train_losses.append(train_loss)
    train_accuracy = correct / total

    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item() * labels.size(0)
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_losses.append(val_loss)
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

model.eval()
test_accuracies = []
correct_test = 0
total_test = 0
with torch.no_grad():
    for images, labels in test_loader:
        images = images.to(device)
        labels = labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        total_test += labels.size(0)
        correct_test += (predicted == labels).sum().item()

test_accuracy = 100 * correct_test / total_test
test_accuracies.append(test_accuracy)
print(f'Test Accuracy: {test_accuracy:.2f}%')

"""### Multi-Scale ViT(패치크기 16,8) : 0.5574"""

def process_image(image, target_size=(224, 224)):
    # 이미지 크기 조정
    resized_image = transforms.Resize(target_size)(image)

    # 범위를 [0, 1]로 맞추기 위해 ToTensor 사용
    tensor_image = transforms.ToTensor()(resized_image)

    # 이미지 데이터를 [0, 1] 범위로 클램핑
    tensor_image = torch.clamp(tensor_image, 0, 1)

    return tensor_image

# 얘네는 픽셀값을 [0,1] 범위로 자동 정규화해야 함.. 모델이 그렇대

# Example use in DataLoader
data_dir = "/content/drive/MyDrive/Data"

# 데이터셋 변환 설정
transform = transforms.Compose([
    transforms.Lambda(lambda img: process_image(img)),
])

# 데이터셋 생성
dataset = ImageFolder(root=data_dir, transform=transform)

# 데이터셋 분할
train_size = int(0.6 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# 데이터 로더 설정
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Multi-Scale ViT 구현
class MultiScaleViT(nn.Module):
    def __init__(self, encoder, scales=[16, 8], output_dim=128):
        super(MultiScaleViT, self).__init__()
        self.encoders = nn.ModuleList([copy.deepcopy(encoder) for _ in scales])
        self.fc = nn.Linear(len(scales) * encoder.config.hidden_size, output_dim)

    def forward(self, image):
        features = []
        for encoder in self.encoders:
            processed_image = processor(images=image, return_tensors="pt").pixel_values.to(image.device)
            feature = encoder(processed_image)[0]  # (batch, seq_len, hidden_size)
            features.append(feature.mean(dim=1))  # (batch, hidden_size)

        combined_features = torch.cat(features, dim=-1)  # (batch, len(scales) * hidden_size)
        output = self.fc(combined_features)  # (batch, output_dim)
        return output

# Classifier 정의
class Classifier(nn.Module):
    def __init__(self, encoder, scales=[16, 8], num_classes=2):
        super(Classifier, self).__init__()
        self.multi_scale_vit = MultiScaleViT(encoder, scales=scales)
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(128, num_classes)  # 최종 분류를 위한 레이어

    def forward(self, x):
        x = self.multi_scale_vit(x)
        x = self.dropout(x)
        x = self.fc(x)
        return x

# 모델 초기화
classifier = Classifier(encoder, scales=[16, 8]).to(device)

# 학습 루프
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=1e-4)

epochs = 10

for epoch in range(epochs):
    classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = classifier(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_accuracy = correct / total

    classifier.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = classifier(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

"""### MultiScaleViT(패치크기 16,8 / 해상도 112,224) : 0.6885

- 다양한 크기의 패치/해상도들을 계층별로 다양하게 사용
- 패치-해상도 연결을 (16, 112) 크고 흐리게, (8, 224) 작고 선명하게 적용
"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
import copy


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def process_image(image, target_size=(224, 224)):
    # 이미지 크기 조정
    resized_image = transforms.Resize(target_size)(image)

    # 범위를 [0, 1]로 맞추기 위해 ToTensor 사용
    tensor_image = transforms.ToTensor()(resized_image)

    # 이미지 데이터를 [0, 1] 범위로 클램핑
    tensor_image = torch.clamp(tensor_image, 0, 1)

    return tensor_image

# 얘네는 픽셀값을 [0,1] 범위로 자동 정규화해야 함.. 모델이 그렇대

# Example use in DataLoader
data_dir = "/content/drive/MyDrive/Data"

# 데이터셋 변환 설정
transform = transforms.Compose([
    transforms.Lambda(lambda img: process_image(img)),
])

# 데이터셋 생성
dataset = ImageFolder(root=data_dir, transform=transform)

# 데이터셋 분할
train_size = int(0.6 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# 데이터 로더 설정
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Multi-Scale ViT 정의
class MultiScaleViT(nn.Module):
    def __init__(self, encoder, scales=[16, 8], resolutions=[112, 224], output_dim=128):
        super(MultiScaleViT, self).__init__()
        assert len(scales) == len(resolutions), "scales와 resolutions의 길이가 같아야 합니다."

        self.encoders = nn.ModuleList([copy.deepcopy(encoder) for _ in scales])
        self.resolutions = resolutions
        self.fc = nn.Linear(len(scales) * encoder.config.hidden_size, output_dim)

    def forward(self, image):
        features = []
        for encoder, resolution in zip(self.encoders, self.resolutions):
            resized_image = transforms.Resize((resolution, resolution))(image)
            processed_image = processor(images=resized_image, return_tensors="pt").pixel_values.to(image.device)
            feature = encoder(processed_image)[0]  # (batch, seq_len, hidden_size)
            features.append(feature.mean(dim=1))  # (batch, hidden_size)

        combined_features = torch.cat(features, dim=-1)  # (batch, len(scales) * hidden_size)
        output = self.fc(combined_features)  # (batch, output_dim)
        return output

# Classifier 정의
class Classifier(nn.Module):
    def __init__(self, encoder, scales=[16, 8], resolutions=[112, 224], output_dim=128, num_classes=2):
        super(Classifier, self).__init__()
        self.multi_scale_vit = MultiScaleViT(encoder, scales=scales, resolutions=resolutions, output_dim=output_dim)
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(output_dim, num_classes)  # output_dim과 num_classes에 맞춰 최종 분류 레이어 정의

    def forward(self, x):
        x = self.multi_scale_vit(x)
        x = self.dropout(x)
        x = self.fc(x)
        return x

# 모델 초기화
classifier = Classifier(encoder, scales=[16, 8], resolutions=[112, 224], output_dim=128, num_classes=2).to(device)

# 학습 루프
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=1e-4)

epochs = 50

for epoch in range(epochs):
    classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = classifier(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_accuracy = correct / total

    classifier.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = classifier(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

epochs = 10

for epoch in range(epochs):
    classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = classifier(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_accuracy = correct / total

    classifier.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = classifier(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

"""### MultiScaleViT(패치크기 32,16,8 / 해상도 56,112,224) -> 깁업"""

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
import copy


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def process_image(image, target_size=(224, 224)):
    # 이미지 크기 조정
    resized_image = transforms.Resize(target_size)(image)

    # 범위를 [0, 1]로 맞추기 위해 ToTensor 사용
    tensor_image = transforms.ToTensor()(resized_image)

    # 이미지 데이터를 [0, 1] 범위로 클램핑
    tensor_image = torch.clamp(tensor_image, 0, 1)

    return tensor_image  # Tensor로 반환

# Example use in DataLoader
data_dir = "/content/drive/MyDrive/Data"

# 데이터셋 변환 설정
transform = transforms.Compose([
    transforms.Lambda(lambda img: process_image(img)),
])

# 데이터셋 생성
dataset = ImageFolder(root=data_dir, transform=transform)

# 데이터셋 분할
train_size = int(0.6 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# 데이터 로더 설정
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Multi-Scale ViT 정의
class MultiScaleViT(nn.Module):
    def __init__(self, encoder, scales=[32, 16, 8], resolutions=[56, 112, 224], output_dim=128):
        super(MultiScaleViT, self).__init__()
        assert len(scales) == len(resolutions), "scales와 resolutions의 길이가 같아야 합니다."

        self.encoders = nn.ModuleList([copy.deepcopy(encoder) for _ in scales])
        self.resolutions = resolutions
        self.fc = nn.Linear(len(scales) * encoder.config.hidden_size, output_dim)

    def forward(self, image):
      features = []
      for encoder, resolution in zip(self.encoders, self.resolutions):
          resized_image = transforms.Resize((resolution, resolution))(image)

          # 배치 차원이 없는 경우 배치 차원을 추가합니다.
          if len(resized_image.shape) == 3:
              resized_image = resized_image.unsqueeze(0)

          processed_image = resized_image.to(image.device)  # 배치 차원 유지된 상태로 사용
          feature = encoder(processed_image)[0]  # (batch, seq_len, hidden_size)
          features.append(feature.mean(dim=1))  # (batch, hidden_size)

      combined_features = torch.cat(features, dim=-1)  # (batch, len(scales) * hidden_size)
      output = self.fc(combined_features)  # (batch, output_dim)
      return output

# Classifier 정의
class Classifier(nn.Module):
    def __init__(self, encoder, scales=[32, 16, 8], resolutions=[56, 112, 224], output_dim=128, num_classes=2):
        super(Classifier, self).__init__()
        self.multi_scale_vit = MultiScaleViT(encoder, scales=scales, resolutions=resolutions, output_dim=output_dim)
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(output_dim, num_classes)  # output_dim과 num_classes에 맞춰 최종 분류 레이어 정의

    def forward(self, x):
        x = self.multi_scale_vit(x)
        x = self.dropout(x)
        x = self.fc(x)
        return x

# 모델 초기화
classifier = Classifier(encoder, scales=[32, 16, 8], resolutions=[56, 112, 224], output_dim=128, num_classes=2).to(device)

# 학습 루프
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=1e-4)

epochs = 10

for epoch in range(epochs):
    classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = classifier(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_accuracy = correct / total

    classifier.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = classifier(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

"""### Dynamic Token Pooling : 0.6066"""

def process_image(image):
    processed_image = processor(images=image, return_tensors="pt").pixel_values
    return processed_image.squeeze(0)

data_dir = "/content/drive/MyDrive/Data"

# ImageFolder를 통해 데이터셋 생성
dataset = ImageFolder(
    root=data_dir,
    transform=transforms.Lambda(process_image)
)
train_size = int(0.6 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class DynamicTokenPooling(nn.Module):
    def __init__(self, hidden_size, top_k_ratio=0.5): # 상위 50프로만 선택(일단 임의)
        super(DynamicTokenPooling, self).__init__()

        # 가장 중요한 패치를 선택하고, 선택된 패치들의 평균을 사용하여 다음 레이어로
        self.top_k_ratio = top_k_ratio
        self.attention_weights = nn.Linear(hidden_size, 1)  # 각 토큰의 중요도를 계산하는 레이어

    def forward(self, x):
        # x의 shape: (batch_size, seq_len, hidden_size)
        attention_scores = self.attention_weights(x).squeeze(-1)  # (batch_size, seq_len)
        top_k = int(self.top_k_ratio * x.size(1))  # 선택할 토큰의 개수
        _, topk_indices = torch.topk(attention_scores, top_k, dim=1, largest=True, sorted=False)  # 중요도가 높은 top-k 토큰 선택

        batch_indices = torch.arange(x.size(0)).unsqueeze(-1).expand(-1, top_k).to(x.device)
        selected_tokens = x[batch_indices, topk_indices]  # 중요한 토큰만 선택
        return selected_tokens.mean(dim=1)  # 선택된 토큰의 평균을 구해 최종적으로 사용

class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.dynamic_pooling = DynamicTokenPooling(hidden_size=encoder.config.hidden_size)
        self.dropout1 = nn.Dropout(0.1)
        self.fc1 = nn.Linear(encoder.config.hidden_size, 128)  # 입력 크기, 출력 크기 128
        self.relu = nn.ReLU()
        self.dropout2 = nn.Dropout(0.1)
        self.fc2 = nn.Linear(128, 2)  # 2 클래스에 대한 출력

    def forward(self, x):
        x = self.dynamic_pooling(x)  # Dynamic Token Pooling 적용
        x = self.dropout1(x)
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)
        return x

class SelectOutput(nn.Module):
    def forward(self, x):
        return x[0]

classifier = Classifier()
model = nn.Sequential(encoder, SelectOutput(), classifier).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=1e-4)

epochs = 10

train_losses = []
val_losses = []

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_losses.append(train_loss)
    train_accuracy = correct / total

    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_losses.append(val_loss)
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

"""### Hybrid(MuliScale ver.2 + Dynamic) : 0.6230"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
import copy

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def process_image(image, target_size=(224, 224)):
    # 이미지 크기 조정
    resized_image = transforms.Resize(target_size)(image)

    # 이미지를 [0, 1] 범위로 자동 정규화
    tensor_image = transforms.ToTensor()(resized_image)

    return tensor_image

data_dir = "/content/drive/MyDrive/Data"

dataset = ImageFolder(
    root=data_dir,
    transform=transforms.Lambda(lambda img: process_image(img))
)

train_size = int(0.6 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class MultiScaleViT(nn.Module):
    def __init__(self, encoder, scales=[16, 8], resolutions=[112, 224], output_dim=128):
        super(MultiScaleViT, self).__init__()
        assert len(scales) == len(resolutions), "scales와 resolutions의 길이가 같아야 합니다."

        self.encoders = nn.ModuleList([copy.deepcopy(encoder) for _ in scales])
        self.resolutions = resolutions
        self.fc = nn.Linear(len(scales) * encoder.config.hidden_size, output_dim)

    def forward(self, image):
        features = []
        for encoder, resolution in zip(self.encoders, self.resolutions):
            resized_image = transforms.Resize((resolution, resolution))(image)
            processed_image = processor(images=resized_image, return_tensors="pt", do_rescale=False).pixel_values.to(image.device)
            feature = encoder(processed_image)[0]  # (batch, seq_len, hidden_size)
            features.append(feature)  # 시퀀스 차원을 유지함

        combined_features = torch.cat(features, dim=1)  # (batch, combined_seq_len, hidden_size)
        return combined_features  # 3차원 텐서를 반환

class DynamicTokenPooling(nn.Module):
    def __init__(self, hidden_size, top_k_ratio=0.5):
        super(DynamicTokenPooling, self).__init__()
        self.top_k_ratio = top_k_ratio
        self.attention_weights = nn.Linear(hidden_size, 1)  # (hidden_size -> 1)

    def forward(self, x):
        # x의 차원이 (batch_size, seq_len, hidden_size)인지 확인
        assert len(x.shape) == 3, "Input tensor x should have 3 dimensions (batch_size, seq_len, hidden_size)"

        # attention_weights를 통해 중요도 점수 계산
        attention_scores = self.attention_weights(x)  # (batch_size, seq_len, 1)
        attention_scores = attention_scores.squeeze(-1)  # (batch_size, seq_len)

        seq_len = attention_scores.size(1)
        top_k = min(int(self.top_k_ratio * seq_len), seq_len)  # top_k 값을 seq_len 이하로 제한

        if top_k == 0:
            top_k = 1  # 최소한 하나의 토큰은 선택되도록 보장

        # 상위 top_k개의 토큰을 선택
        _, topk_indices = torch.topk(attention_scores, top_k, dim=1, largest=True, sorted=False)

        # 선택된 토큰들에 대한 인덱스 생성 및 선택
        batch_indices = torch.arange(x.size(0)).unsqueeze(-1).expand(-1, top_k).to(x.device)
        selected_tokens = x[batch_indices, topk_indices]  # (batch_size, top_k, hidden_size)

        # 선택된 토큰들의 평균을 구하여 최종 출력
        return selected_tokens.mean(dim=1)  # (batch_size, hidden_size)

class Classifier(nn.Module):
    def __init__(self, encoder, scales=[16, 8], resolutions=[112, 224], num_classes=2):
        super(Classifier, self).__init__()
        self.multi_scale_vit = MultiScaleViT(encoder, scales=scales, resolutions=resolutions)
        self.dynamic_pooling = DynamicTokenPooling(hidden_size=encoder.config.hidden_size)
        self.fc = nn.Linear(encoder.config.hidden_size, num_classes)  # 최종 분류를 위한 레이어

    def forward(self, x):
        x = self.multi_scale_vit(x)
        x = self.dynamic_pooling(x)  # Dynamic Token Pooling 적용
        x = self.fc(x)
        return x

# 모델 초기화
classifier = Classifier(encoder, scales=[16, 8], resolutions=[112, 224]).to(device)

# 학습 설정
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=1e-4)

epochs = 10

for epoch in range(epochs):
    classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = classifier(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_accuracy = correct / total

    classifier.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = classifier(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

"""### Baseline with patch size 8 : 0.6728"""

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader, random_split
from torchvision import transforms
import copy

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def process_image(image, target_size=(224, 224)):
    # 이미지 크기 조정
    resized_image = transforms.Resize(target_size)(image)

    # 범위를 [0, 1]로 맞추기 위해 ToTensor 사용
    tensor_image = transforms.ToTensor()(resized_image)

    # 이미지 데이터를 [0, 1] 범위로 클램핑
    tensor_image = torch.clamp(tensor_image, 0, 1)

    return tensor_image

# Example use in DataLoader
data_dir = "/content/drive/MyDrive/Data"

# 데이터셋 변환 설정
transform = transforms.Compose([
    transforms.Lambda(lambda img: process_image(img)),
])

# 데이터셋 생성
dataset = ImageFolder(root=data_dir, transform=transform)

# 데이터셋 분할
train_size = int(0.6 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

# 데이터 로더 설정
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# Multi-Scale ViT 구현
class MultiScaleViT(nn.Module):
    def __init__(self, encoder, scales=8, output_dim=128):
        super(MultiScaleViT, self).__init__()
        self.encoders = nn.ModuleList([copy.deepcopy(encoder) for _ in scales])
        self.fc = nn.Linear(len(scales) * encoder.config.hidden_size, output_dim)

    def forward(self, image):
        features = []
        for encoder in self.encoders:
            processed_image = processor(images=image, return_tensors="pt").pixel_values.to(image.device)
            feature = encoder(processed_image)[0]  # (batch, seq_len, hidden_size)
            features.append(feature.mean(dim=1))  # (batch, hidden_size)

        combined_features = torch.cat(features, dim=-1)  # (batch, len(scales) * hidden_size)
        output = self.fc(combined_features)  # (batch, output_dim)
        return output

# Classifier 정의
class Classifier(nn.Module):
    def __init__(self, encoder, scales=8, num_classes=2):
        super(Classifier, self).__init__()
        self.multi_scale_vit = MultiScaleViT(encoder, scales=scales)
        self.dropout = nn.Dropout(0.1)
        self.fc = nn.Linear(128, num_classes)  # 최종 분류를 위한 레이어

    def forward(self, x):
        x = self.multi_scale_vit(x)
        x = self.dropout(x)
        x = self.fc(x)
        return x

# 모델 초기화
classifier = Classifier(encoder, scales=[8]).to(device)

# 학습 루프
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(classifier.parameters(), lr=1e-4)

epochs = 10

for epoch in range(epochs):
    classifier.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = classifier(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_accuracy = correct / total

    classifier.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = classifier(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

"""### 로컬 어텐션 메커니즘 : 0.5738

- ViT는 기본적으로 전역적 어텐션을 사용하여 이미지의 모든 패치 사이의 관계를 학습
- 그런데, 이건 로컬 패턴(작은 영역에서의 세부적 정보)을 학습하는 데 비효율적일 수 있음
- 따라서, 로컬 어텐션 메커니즘을 도입해서 특정 패치 주변의 패치들과만 어텐션을 주고받는 방식으로
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision.datasets import ImageFolder
from torchvision import transforms
from transformers import ViTModel, ViTConfig, ViTFeatureExtractor

class LocalAttention(nn.Module):
    def __init__(self, window_size, embed_dim, num_heads):
        super(LocalAttention, self).__init__()
        self.window_size = window_size
        self.num_heads = num_heads
        self.embed_dim = embed_dim
        self.attn = nn.MultiheadAttention(embed_dim, num_heads)

    def forward(self, x):
        # Input x shape: (batch_size, num_patches, embed_dim)
        bsz, num_patches, embed_dim = x.size()
        window_size = min(self.window_size, num_patches)
        local_attn_output = []

        # Iterate through the sequence with a sliding window
        for i in range(0, num_patches, window_size):
            start_idx = i
            end_idx = min(i + window_size, num_patches)
            local_patches = x[:, start_idx:end_idx, :]  # Select local patches
            attn_output, _ = self.attn(local_patches, local_patches, local_patches)
            local_attn_output.append(attn_output)

        # Concatenate all local attention outputs
        local_attn_output = torch.cat(local_attn_output, dim=1)

        return local_attn_output

class CustomViTEncoder(nn.Module):
    def __init__(self, model_name, window_size):
        super(CustomViTEncoder, self).__init__()
        self.vit = ViTModel.from_pretrained(model_name)
        self.local_attention = LocalAttention(window_size, self.vit.config.hidden_size, self.vit.config.num_attention_heads)

    def forward(self, x):
        x = self.vit.embeddings(x)
        x = self.local_attention(x)
        return x

# 모델 초기화
window_size = 16  # 로컬 어텐션 윈도우 크기
custom_encoder = CustomViTEncoder("google/vit-base-patch16-224-in21k", window_size)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

feature_extractor = ViTFeatureExtractor.from_pretrained("google/vit-base-patch16-224-in21k")

def process_image(image):
    processed_image = feature_extractor(images=image, return_tensors="pt").pixel_values
    return processed_image.squeeze(0)

data_dir = "/content/drive/MyDrive/Data"  # 여기에 데이터 경로 설정

dataset = ImageFolder(
    root=data_dir,
    transform=transforms.Lambda(process_image)
)

train_size = int(0.6 * len(dataset))
val_size = int(0.2 * len(dataset))
test_size = len(dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

class Classifier(nn.Module):
    def __init__(self):
        super(Classifier, self).__init__()
        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)
        self.dropout1 = nn.Dropout(0.1)
        self.fc1 = nn.Linear(768, 128)  # 입력 크기 768, 출력 크기 128
        self.relu = nn.ReLU()
        self.dropout2 = nn.Dropout(0.1)
        self.fc2 = nn.Linear(128, 2)  # 2 클래스에 대한 출력

    def forward(self, x):
        x = x.transpose(1, 2)  # (batch_size, embed_dim, num_patches + 1)
        x = self.global_avg_pool(x).squeeze(-1)  # (batch_size, embed_dim)
        x = self.dropout1(x)
        x = self.fc1(x)  # (batch_size, 128)
        x = self.relu(x)
        x = self.dropout2(x)
        x = self.fc2(x)  # (batch_size, 2)
        return x

window_size = 16  # 로컬 어텐션 윈도우 크기
custom_encoder = CustomViTEncoder("google/vit-base-patch16-224-in21k", window_size)

model = nn.Sequential(custom_encoder, Classifier()).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)

epochs = 10

train_losses = []
val_losses = []

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for inputs, labels in train_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    train_loss = running_loss / total
    train_losses.append(train_loss)
    train_accuracy = correct / total

    model.eval()
    val_loss = 0.0
    val_correct = 0
    val_total = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)

            outputs = model(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs, 1)
            val_total += labels.size(0)
            val_correct += (predicted == labels).sum().item()

    val_loss /= val_total
    val_losses.append(val_loss)
    val_accuracy = val_correct / val_total

    print(f'Epoch {epoch+1}/{epochs}, '
          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '
          f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

"""### 다음

*다 뻥이었어*

- Validation에 따라서 그냥 다 달라 흥
- 아아아ㅏ아ㅏ각
"""

